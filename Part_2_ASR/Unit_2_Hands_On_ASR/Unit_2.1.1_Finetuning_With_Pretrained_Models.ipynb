{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YYBe0EScJ15"
   },
   "source": [
    "### Training with Pretrained Models\n",
    "\n",
    "In this tutorial, we will explore ASR training with pretrained models. Pretrained models are a shortcut to training high accuracy models even with limited data or compute resources.\n",
    "\n",
    "These models have been trained on large-scale audio-text datasets—often consisting of thousands of hours of transcribed speech—by major research groups or organizations. As a result, they have already learned rich acoustic and linguistic representations. By starting from a pretrained model, we can fine-tune it on a smaller, domain-specific dataset (such as customer service calls, lecture recordings, or podcasts), allowing it to adapt to the new data much faster and with better performance than training from scratch.\n",
    "\n",
    "This approach is a form of transfer learning, where the pretrained model transfers general knowledge (e.g., how speech sounds map to words) to a specific downstream task or domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yiV6QJveyP8"
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 116136,
     "status": "ok",
     "timestamp": 1746550895081,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "LhBCRJGgpi1n",
    "outputId": "5a50f697-abbf-47d7-bbdf-a50e0fbd3a6c"
   },
   "outputs": [],
   "source": [
    "!pip install datasets accelerate librosa evaluate jiwer speechbrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eORyR3NEe0PL"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746555035725,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "dgwRuMmqujCL"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHk_sNCDxb62"
   },
   "outputs": [],
   "source": [
    "# This is used for visualising the data nicely\n",
    "\n",
    "def array_to_audio_html(array, rate=16000):\n",
    "    # Save array directly to an in-memory buffer\n",
    "    buffer = io.BytesIO()\n",
    "    sf.write(buffer, array, rate, format='wav')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Encode to base64\n",
    "    b64_audio = base64.b64encode(buffer.read()).decode('utf-8')\n",
    "\n",
    "    # Create HTML audio tag\n",
    "    return f'<audio controls src=\"data:audio/wav;base64,{b64_audio}\"></audio>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG_gvXOue5KC"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AQvxJ_ye8UM"
   },
   "source": [
    "### Setup and Load Data\n",
    "\n",
    "We load `atco2-asr`, which is an air traffic control dataset uploaded on HuggingFace.\n",
    "\n",
    "For the purposes of teaching, we will omit the following steps, and show some of those steps in future tutorials\n",
    "\n",
    "1. Removing noise: The ATCO data is terribly noisy, and contains a lot of artifacts, which you should remove\n",
    "1. Further preprocessing and augmentation: If possible, see how libraries like `audiomentations` or `speechbrain` can help preprocess and augment your already existing data. This is especially so for small datasets like `atco2-asr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm24jOIYuyj-"
   },
   "outputs": [],
   "source": [
    "# 1. Load ATCO2‑ASR from Hugging Face\n",
    "dataset = load_dataset(\"jlvdoorn/atco2-asr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U0fIPu_f_RF"
   },
   "source": [
    "### Exploring the data:\n",
    "\n",
    "We do a brief exploration of the dataset, showing what columns belong in the dataset and an audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1746555109174,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "XRXrIYEvwOmy",
    "outputId": "9857e28c-5f99-4f34-f668-b93496d0517a"
   },
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Check the available splits\n",
    "print(dataset)\n",
    "\n",
    "# Peek at the first training example\n",
    "print(\"\\n First training example:\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# List available columns\n",
    "print(\"\\n Columns in each example:\", dataset[\"train\"].column_names)\n",
    "\n",
    "# Check the type of audio column\n",
    "print(\"\\n Audio column type:\", type(dataset[\"train\"][0][\"audio\"]))\n",
    "\n",
    "# Check the contents of the audio column\n",
    "print(\"\\n Keys in each example['audio']:\", dataset[\"train\"][0][\"audio\"].keys())\n",
    "\n",
    "# Show audio metadata\n",
    "audio_sample = dataset[\"train\"][0][\"audio\"]\n",
    "print(\"\\n Audio metadata:\")\n",
    "print(\"  Sampling rate:\", audio_sample[\"sampling_rate\"])\n",
    "print(\"  Num samples  :\", len(audio_sample[\"array\"]))\n",
    "print(\"  Duration (s) :\", len(audio_sample[\"array\"]) / audio_sample[\"sampling_rate\"])\n",
    "\n",
    "# Play the audio (Jupyter / Colab only)\n",
    "ipd.Audio(audio_sample[\"array\"], rate=audio_sample[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCLKx3RWkPcl"
   },
   "source": [
    "### But can we do better?\n",
    "\n",
    "The previous output was rather messy, and showed a lot of unrelated data that we might not need. Let's use the outputs from the previous cell to summarize everything into a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "executionInfo": {
     "elapsed": 1825,
     "status": "ok",
     "timestamp": 1746555127895,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "ZuUbjVSpuGaV",
    "outputId": "4eaaa513-1016-4a62-c1ba-2fd6fcf18877"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import soundfile as sf\n",
    "\n",
    "def show_random_elements(dataset, num_examples = 10):\n",
    "  assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset\"\n",
    "  picks = []\n",
    "  for _ in range(num_examples):\n",
    "    pick = random.randint(0, len(dataset) - 1)\n",
    "    while pick in picks:\n",
    "      pick = random.randint(0, len(dataset) - 1)\n",
    "    picks.append(pick)\n",
    "\n",
    "  df = pd.DataFrame(dataset[picks]['audio'])\n",
    "  df2 = pd.DataFrame(dataset[picks]['text'], columns=['text'])\n",
    "  df = pd.concat([df, df2], axis=1)\n",
    "  # Function to convert audio arrays directly to HTML audio elements\n",
    "\n",
    "\n",
    "  # Apply the conversion to the DataFrame\n",
    "  df['audio'] = df.apply(lambda row: array_to_audio_html(row['array'], row['sampling_rate']), axis=1)\n",
    "  html_table = df[['audio','text', 'sampling_rate']].to_html(escape=False)\n",
    "\n",
    "  display(HTML(html_table))\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0yx1rhHkau9"
   },
   "source": [
    "This is our safety net: Even though we have seen that most of the data is in 16000HZ, it is always good to just cast everything to 16k Hz again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746555137475,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "nLwUqoPvviyh"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))  # ensure consistent sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRPf_3CukkuQ"
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1392,
     "status": "ok",
     "timestamp": 1746554916774,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "_12vs1IqvoVH",
    "outputId": "762328e7-2a8a-427b-b849-47e3a0246adb"
   },
   "outputs": [],
   "source": [
    "# 2. Load pre-trained Wav2Vec2 and its processor\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model     = Wav2Vec2ForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8H61QDY6kmv9"
   },
   "source": [
    "### Preprocessing\n",
    "This helps us remove special characters, as Wav2Vec2 does not know what punctuation is.\n",
    "\n",
    "This makes sense, as you cannot really identify where punctuation etc should be based off speech. Of course, recent advancements have made it possible for ASR models to also reason where to add punctuation, but we omit it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "cd57eae43d714879bb2508573407b007",
      "8850295e8e7a49d7aa4c44f10c159f30",
      "b4202983a05a471989e35989521325d2",
      "024e60d8fb024372bfc437e60cfe8ebe",
      "8f6548a0b9af45258bdb6aa2c6d91b29",
      "908dc0dbf7974d10b12b3bad2a8e07bd",
      "94407caf514e4f5d8d4fd8b64014b1fa",
      "4757c46a2836417f9014921a9c454aea",
      "afb5bde890df4b219b08970e3206d978",
      "1f751445af994284be04a07f0c29c998",
      "a3bdd36c5b734e29abce6bf33516bfdb",
      "af93dc17243342178cd0cfa397981687",
      "8536bed9cc694793b5f375428fb7836d",
      "87f5ea872acf478dbada8dee067d8303",
      "a3945a78da0d474bacff85410acf942b",
      "4b51399a133d4d9eaf020c80f4678b31",
      "0192758a045845f482be1fac55f7a0fa",
      "2cf33d7b48034a2d871c0b49633c5296",
      "852ef909d741481ca05d51aaf88d757e",
      "ce7090cefa72454284e06d952dfcaf1d",
      "8f979dd137e941f68f5629b43191537c",
      "7c877931a3094547b2cacde5f09b2aea"
     ]
    },
    "executionInfo": {
     "elapsed": 5860,
     "status": "ok",
     "timestamp": 1746555185262,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "2yRd_5pdtxdb",
    "outputId": "a4da6cab-ac30-4226-f716-6cf6a73e7ae9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(remove_special_characters) # This applies the function to every row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrZw2o0Zk-K7"
   },
   "source": [
    "### Dataset preparation\n",
    "\n",
    "We do two things here:\n",
    "1. Extract audio features from our audio array, using Wav2Vec2's feature processor\n",
    "1. We tokenize the labels for loss computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746555191290,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "9BMrVz3TzYNx"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    '''\n",
    "    Ideally we use the processor directly here. But using the processor will lead to\n",
    "    a bug in HuggingFace complaining we are assinging multiple values to a single key.\n",
    "    '''\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Process audio inputs - only pass return_attention_mask to the feature_extractor\n",
    "    features = processor.feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False, #return True if you want the attention mask\n",
    "        do_normalize = True\n",
    "    )\n",
    "    batch[\"input_values\"] = features.input_values[0]\n",
    "\n",
    "    # If you need the attention mask as well\n",
    "    if hasattr(features, \"attention_mask\"):\n",
    "        batch[\"attention_mask\"] = features.attention_mask[0]\n",
    "\n",
    "    # Process text targets separately\n",
    "    # with processor.as_target_processor():\n",
    "    #     batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5whp6sygzdmo"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asTVoiFslWAM"
   },
   "source": [
    "### Data Collator\n",
    "\n",
    "Before training, there is a very important thing we must do: Collate the data.\n",
    "\n",
    "In our data right now, there exists samples of different lengths, and this means our training will FAIL as we cannot form proper tensors with such jagged data. To circumvent this, we need to properly pad our data, and also mask all padding tokens during training (We do not want the model to learn how to add padding)\n",
    "\n",
    "To do this we create a data collator that does this for us, using the processor's inbuilt capabilities to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYcphMhxzg6M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACnqFCi52Gu_"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgMNe7X0mAFZ"
   },
   "source": [
    "###  Word error rate\n",
    "\n",
    " We calculate the word error rate for training - This will be discussed more in the next notebook, but is used here to illustrate how to train models with some custom metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1746555049644,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "YLarEyE52LX_"
   },
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTy5nZIuml6F"
   },
   "source": [
    "### Freezing layers\n",
    "\n",
    "Freezing means to prevent the model from updating a set of layers during training - This is good in some cases, as we might not want the model to erroneously update some parts of the model that we know are trained well enough\n",
    "\n",
    "For example, Wav2Vec2 was trained on thousands of hours of ASR data, and thus we can rest assured that the feature extractor should extract important auditory features\n",
    "\n",
    "We should focus our attention more towards tuning the intermediate layers in order to adapt it to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746552838249,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "ollZbqt22dxt",
    "outputId": "bf2e4081-bad6-4da8-bd84-9ff101b345b7"
   },
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1N0WSrUnCW6"
   },
   "source": [
    "### Training arguments:\n",
    "These arguments define our training process, and include useful hyperparameters that you might want to tune to improve the accuracy of your models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1746555310568,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "KzcF6L_m2NRC"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  group_by_length=True, #Groups examples of similar length together to reduce padding and improve efficiency\n",
    "  per_device_train_batch_size=16, #Batch Size\n",
    "  gradient_accumulation_steps=4, #Accumulates gradient, so total steps before updating is 16 * 4 = effective batch size of 64\n",
    "  num_train_epochs=10, # Change this if you want to seriously evalmaxx on airplane conversations\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True, #Reduce memory usage at the cost of speed\n",
    "  save_steps = 200,\n",
    "  logging_strategy = \"epoch\",\n",
    "  eval_strategy=\"epoch\",\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "  report_to= \"none\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1746554037353,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "8EPwgy7S2hxM"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "#Add everything we defined previously into the trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 695722,
     "status": "ok",
     "timestamp": 1746554733858,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "_MYFpF2o2k_7",
    "outputId": "b583c35b-fc6c-4169-aab0-6b5e3e3ee140"
   },
   "outputs": [],
   "source": [
    "trainer.train() #Just a single line, how useful is that!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8MgRtoIzktN"
   },
   "source": [
    "The training loss and the validation loss decreased as the number epochs increased. This indicates that the model is learning to better predict the training and validation transcription.\n",
    "\n",
    "However, the word error rate (WER) stayed roughly the same. One reason is because we did not do hyperparameter tuning to help the model generalise better. We also only trained it for 10 epochs. Additionally, small changes in the model predictions might not affect the WER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQpaHGldsdOc"
   },
   "source": [
    "### Inference\n",
    "\n",
    "Always test what your model outputs after training - You want to make sure it is not overfitting or hacking the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1746555351161,
     "user": {
      "displayName": "Warren Low",
      "userId": "03832505700203327791"
     },
     "user_tz": -480
    },
    "id": "Gzxvv6Ac84Kd",
    "outputId": "c2233993-9efc-4182-a93e-57122762a6aa"
   },
   "outputs": [],
   "source": [
    "#Inference with the model\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i in range(5):\n",
    "        batch = dataset[i]['audio']['array']\n",
    "        input_values = processor(batch, return_tensors=\"pt\", padding=True, sampling_rate = 16000).input_values\n",
    "        with torch.no_grad():\n",
    "            model = model.to(\"cuda\")\n",
    "            logits = model(input_values.to('cuda')).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        pred_str = processor.batch_decode(pred_ids[0])\n",
    "        predictions.append(pred_str)\n",
    "        references.append(dataset[i]['text'])\n",
    "    return predictions, references\n",
    "\n",
    "predictions, references = evaluate(model, dataset[\"validation\"])\n",
    "\n",
    "#Print dataframe\n",
    "df = pd.DataFrame({'predictions': predictions, 'references': references})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3_O5VpntUfa"
   },
   "source": [
    "Notice the outputs are still quite bad\n",
    "\n",
    "- Note that this dataset is extremely noisy, and highlights the need for us to denoise our inputs beforehand\n",
    "\n",
    "- We are also constrained by Google Colab here\n",
    "  - Google Colab has a timeout after 30 mins to 1 hour\n",
    "  - Especially so for GPU instances\n",
    "\n",
    " By right we should increase the num of epochs until the loss plateaus, but that requires a dedicated compute resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kN0hlr6bz_Ih"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0192758a045845f482be1fac55f7a0fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "024e60d8fb024372bfc437e60cfe8ebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f751445af994284be04a07f0c29c998",
      "placeholder": "​",
      "style": "IPY_MODEL_a3bdd36c5b734e29abce6bf33516bfdb",
      "value": " 446/446 [00:05&lt;00:00, 77.67 examples/s]"
     }
    },
    "1f751445af994284be04a07f0c29c998": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cf33d7b48034a2d871c0b49633c5296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4757c46a2836417f9014921a9c454aea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b51399a133d4d9eaf020c80f4678b31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c877931a3094547b2cacde5f09b2aea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "852ef909d741481ca05d51aaf88d757e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8536bed9cc694793b5f375428fb7836d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0192758a045845f482be1fac55f7a0fa",
      "placeholder": "​",
      "style": "IPY_MODEL_2cf33d7b48034a2d871c0b49633c5296",
      "value": "Map: 100%"
     }
    },
    "87f5ea872acf478dbada8dee067d8303": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_852ef909d741481ca05d51aaf88d757e",
      "max": 113,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce7090cefa72454284e06d952dfcaf1d",
      "value": 113
     }
    },
    "8850295e8e7a49d7aa4c44f10c159f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_908dc0dbf7974d10b12b3bad2a8e07bd",
      "placeholder": "​",
      "style": "IPY_MODEL_94407caf514e4f5d8d4fd8b64014b1fa",
      "value": "Map: 100%"
     }
    },
    "8f6548a0b9af45258bdb6aa2c6d91b29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f979dd137e941f68f5629b43191537c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "908dc0dbf7974d10b12b3bad2a8e07bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94407caf514e4f5d8d4fd8b64014b1fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3945a78da0d474bacff85410acf942b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f979dd137e941f68f5629b43191537c",
      "placeholder": "​",
      "style": "IPY_MODEL_7c877931a3094547b2cacde5f09b2aea",
      "value": " 113/113 [00:00&lt;00:00, 598.87 examples/s]"
     }
    },
    "a3bdd36c5b734e29abce6bf33516bfdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af93dc17243342178cd0cfa397981687": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8536bed9cc694793b5f375428fb7836d",
       "IPY_MODEL_87f5ea872acf478dbada8dee067d8303",
       "IPY_MODEL_a3945a78da0d474bacff85410acf942b"
      ],
      "layout": "IPY_MODEL_4b51399a133d4d9eaf020c80f4678b31"
     }
    },
    "afb5bde890df4b219b08970e3206d978": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b4202983a05a471989e35989521325d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4757c46a2836417f9014921a9c454aea",
      "max": 446,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_afb5bde890df4b219b08970e3206d978",
      "value": 446
     }
    },
    "cd57eae43d714879bb2508573407b007": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8850295e8e7a49d7aa4c44f10c159f30",
       "IPY_MODEL_b4202983a05a471989e35989521325d2",
       "IPY_MODEL_024e60d8fb024372bfc437e60cfe8ebe"
      ],
      "layout": "IPY_MODEL_8f6548a0b9af45258bdb6aa2c6d91b29"
     }
    },
    "ce7090cefa72454284e06d952dfcaf1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
