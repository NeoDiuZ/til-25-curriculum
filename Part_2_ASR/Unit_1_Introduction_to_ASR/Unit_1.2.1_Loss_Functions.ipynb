{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Loss functions for ASR\n","\n","Automatic speech recognition (ASR) systems convert spoken language into text. Most ASR systems rely heavily on deep learning models that learn from audio data by minimizing a loss function. (If you're not sure what a loss function is, check out [Unit 1.1.2.2](https://docs.google.com/presentation/d/1cX3o6SH1fc4cTzUmhZvyYti557utwgRlgetn6ao_6tg/view#slide=id.g34ea0365488_0_117).)\n","\n","Designing a loss function for ASR is complicated by the fact that input audio and output texts almost always have different lengths, and are not aligned - i.e. there is no one-to-one correspondence between audio samples and output words/characters. Two common loss functions are used that circumvent these complications."],"metadata":{"id":"s7ioatj4Y69l"}},{"cell_type":"markdown","source":["## 1. Connectionist temporal classification (CTC) loss\n","\n","CTC loss is designed for sequence-to-sequence tasks where:\n","- there is a many-to-one mapping of input tokens to output tokens, and\n","- the relative ordering of pairs of corresponding input and output tokens is preserved.\n","\n","ASR satisfies both of these conditions, because:\n","- multiple input audio samples / spectrogram timesteps map to a single word or character, and\n","- both input audio and output text are ordered by time.\n","\n","CTC loss is used in several state-of-the-art ASR models, such as DeepSpeech and Wav2Vec."],"metadata":{"id":"RZmR-HRybQ95"}},{"cell_type":"markdown","source":["### How does CTC loss work?\n","\n","1. The many-to-one mapping from input to output is turned into a one-to-one mapping by allowing the model to predict blank tokens as part of the output.\n","2. CTC loss is then calculated by summing the probabilities of all possible alignments between the input and target sequences that result in a correct prediction.\n","\n","A detailed explanation can be found [here](https://distill.pub/2017/ctc/)."],"metadata":{"id":"Iz4aJwPiJrMp"}},{"cell_type":"markdown","source":["### Training a model using CTC loss\n","\n","PyTorch provides a CTC loss implementation at `torch.nn.CTCLoss`. Let's use this to train a simple LSTM model for ASR."],"metadata":{"id":"m481C9_zDwdq"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"pYcZa0n7ZjQA","executionInfo":{"status":"ok","timestamp":1746549941990,"user_tz":-480,"elapsed":98,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}},"outputId":"041b7a4f-e6fc-4aac-a9eb-f9cda8472d1a","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["CTC loss: 5.526454448699951\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","# Create a simple LSTM model to be trained using CTC loss.\n","class SimpleLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # LSTM output shape is (batch, sequence, feature)\n","        outputs, _ = self.lstm(x)\n","        # Reshape output for the fully connected layer\n","        outputs = self.fc(outputs)\n","        # Apply log softmax on the last dimension (num_classes)\n","        return outputs.log_softmax(dim=-1)\n","\n","\n","# Initialize the model, loss function, and optimizer\n","input_size = 13  # input feature dimension, e.g. number of MFCCs\n","hidden_size = 128 # hidden dimension of LSTM model\n","num_layers = 2\n","num_classes = 20 # Including the blank label for CTC\n","\n","model = SimpleLSTM(input_size, hidden_size, num_layers, num_classes)\n","ctc_loss = nn.CTCLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Generate some random data\n","batch_size = 16\n","sequence_length = 50\n","inputs = torch.randn(batch_size, sequence_length, input_size)  # (batch, sequence, feature)\n","input_lengths = torch.full((batch_size,), sequence_length, dtype=torch.long)\n","targets = torch.randint(1, num_classes, (batch_size, 30), dtype=torch.long)\n","target_lengths = torch.randint(10, 30, (batch_size,), dtype=torch.long)\n","\n","# Forward pass: compute predicted outputs by passing inputs to the model\n","logits = model(inputs)  # (batch, sequence, num_classes)\n","logits = logits.transpose(0, 1)  # CTC needs input as (sequence, batch, num_classes)\n","\n","# Calculate loss\n","loss = ctc_loss(logits, targets, input_lengths, target_lengths)\n","\n","# Backward pass: compute gradient of the loss with respect to model parameters\n","loss.backward()\n","\n","# Perform a single optimization step (parameter update)\n","optimizer.step()\n","\n","print(\"CTC loss:\", loss.item())"]},{"cell_type":"markdown","source":["## 2. Cross-entropy (CE) loss\n","CE loss is used for classification tasks, where a model predicts a probability distribution over output classes (e.g. a vocabulary of words or characters). CE loss quantifies the \"difference\" between the predicted and ground-truth probability distributions."],"metadata":{"id":"-hmcwxK_t7_0"}},{"cell_type":"markdown","source":["### How does CE loss work for ASR?\n","\n","Recall that CTC loss aligns input and output sequences by inserting blank tokens in the predicted output.\n","\n","However, alignment is not necessary if we use CE loss with a **transformer model**. Since transformers simply predict one token for each element of the output sequence, the input sequence length doesn't matter!\n","\n","Transformer models also give other advantages - for instance:\n","- Information from previous output tokens can be used to predict the next output token, unlike CTC-based models that assume output tokens are pairwise conditionally independent given the input.\n","- Transformer models can generate output sequences that are longer than the input. (Though this is rarely necessary for ASR.)\n","\n","State-of-the-art ASR models trained with CE loss include Whisper, VITA, Ichigo and Moshi."],"metadata":{"id":"J8rlP9aruTb_"}},{"cell_type":"markdown","source":["### Training a model using CE loss\n","\n","PyTorch provides a CE loss implementation at `torch.nn.CrossEntropyLoss`. Let's use this to train a simple transformer model for ASR."],"metadata":{"id":"A5JxhAYFYSR7"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","# Create a simple transformer model to be trained using CE loss.\n","class SimpleTransformer(nn.Module):\n","    def __init__(self, num_classes, input_dim, hidden_dim):\n","        super().__init__()\n","        self.input_proj = nn.Linear(input_dim, hidden_dim)\n","        self.embedding = nn.Embedding(num_classes, hidden_dim)\n","        self.transformer = nn.Transformer(hidden_dim)\n","        self.output_proj = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, inputs, targets):\n","        transformer_out = self.transformer(\n","            self.input_proj(inputs), self.embedding(targets),\n","        )\n","        return self.output_proj(transformer_out)\n","\n","\n","# Initialize the model, loss function, and optimizer\n","input_dim = 16 # input feature dimension, e.g. number of MFCCs\n","hidden_dim = 512 # hidden dimension of transformer model\n","num_classes = 10\n","\n","model = SimpleTransformer(num_classes, input_dim, hidden_dim)\n","ce_loss = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Generate some random data\n","batch_size = 16\n","inputs = torch.randn(batch_size, input_dim)\n","targets = torch.randint(0, num_classes, (batch_size,))\n","\n","# Forward pass: compute predicted outputs by passing inputs to the model\n","logits = model(inputs, targets)\n","\n","# Calculate loss\n","# Note that when training an actual transformer, we'd shift the targets by 1 step\n","# along the time dimension, because we want to predict the *next* token.\n","loss = ce_loss(logits, targets)\n","\n","# Backward pass: compute gradient of the loss with respect to model parameters\n","loss.backward()\n","\n","# Perform a single optimization step (parameter update)\n","optimizer.step()\n","\n","print(\"CE loss:\", loss.item())"],"metadata":{"id":"kbAu22LeuSqY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746550067877,"user_tz":-480,"elapsed":1480,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}},"outputId":"c3bf18af-b396-4679-e33e-a970260c2d86"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CE loss: 2.4389243125915527\n"]}]},{"cell_type":"markdown","source":["## Further resources\n","\n","### Other loss functions\n","- [RNNT Loss](https://lorenlugosch.github.io/posts/2020/11/transducer/)\n","- [Explanation of CTC architectures](https://huggingface.co/learn/audio-course/en/chapter3/ctc)\n","- [Explanation of Seq2Seq architectures](https://huggingface.co/learn/audio-course/en/chapter3/seq2seq)"],"metadata":{"id":"K8tKn9j85-5C"}}]}