{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torch torchaudio datasets jiwer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sx_3H8xdOfw9","executionInfo":{"status":"ok","timestamp":1746588778110,"user_tz":-480,"elapsed":113470,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}},"outputId":"8ae3fe05-677f-48af-b534-1d8d7f0f77b3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting datasets\n","  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n","Collecting jiwer\n","  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec (from torch)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n","Collecting rapidfuzz>=3.9.7 (from jiwer)\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m847.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, rapidfuzz, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jiwer, nvidia-cusolver-cu12, datasets\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 jiwer-3.1.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rapidfuzz-3.13.0 xxhash-3.5.0\n"]}]},{"cell_type":"markdown","source":["## A simple cross attention transformer\n","\n","As a recap, we use cross attention transformers in situations where the input and output are of different types.\n","\n","For example, in machine translation between 2 languages, input English and output Japanese could attend to each other. In ASR, the input audio and the output text could attend to each other.\n","\n","In self-attention, we capture intra‑sequence dependencies (e.g. which English word helps predict the next English word).\n","\n","In cross-attention, we capture inter‑sequence dependencies (e.g. which audio frame aligns to this text token).\n","\n","In the context of ASR,\n","\n","*   Source: raw audio waveform → converted to mel‑spectrogram frames → embedded and passed through the encoder.\n","*   Encoder memory: a sequence of vectors representing spectral patterns over time.\n","* Decoder self‑attention: For example, the decoder (produces text) can learn that the “h” matters more than “t” when deciding what comes next.\n","* Decoder cross‑attention: each decoder step asks “which audio frames in the spectrogram correspond to the next character or word I should produce?”\n","\n","In short, cross‑attention learns alignments between sound patterns and textual units.\n","\n","----\n","\n","In the example below, we implement three sub‑layers of a Transformer‑decoder layer. (So, it's not just the cross attention itself).\n","1. Self‑attention over the target sequence\n","2. Cross‑attention to the encoder’s outputs (the “memory”)\n","3. Position‑wise feed‑forward network\n","with residual connections and layer‑norms after each.\n","\n","For a full decoder (which we did not show below), you would also require\n","1. A token embedding + positional encoding step before feeding tokens into this block\n","2. A mask on the self‑attention so each position can only attend to past (or past+present) tokens during training\n","3. A stack of N such CrossAttentionBlocks (typically 6–12 layers)\n","4. A final linear projection and softmax to map each position’s output to vocabulary logits"],"metadata":{"id":"Ieyhd9nMHEFA"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# -----------------------------------------\n","# Cross-Attention Transformer\n","# -----------------------------------------\n","# This module demonstrates a minimal cross-attention block,\n","# showing how a target sequence \"attends\" to a source sequence.\n","# We use PyTorch's built-in MultiheadAttention for simplicity.\n","\n","class CrossAttentionBlock(nn.Module):\n","    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):\n","        \"\"\"\n","        Initializes the CrossAttentionBlock.\n","\n","        Args:\n","            d_model (int): Dimensionality of input embeddings.\n","            nhead (int): Number of attention heads.\n","            dropout (float): Dropout probability.\n","        \"\"\"\n","        super().__init__()\n","        # 1) Self-attention layer: target attends to itself\n","        self.self_attn  = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n","        # 2) Cross-attention layer: target attends to source (memory)\n","        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n","        # Position-wise feedforward network\n","        self.linear1    = nn.Linear(d_model, d_model * 4)\n","        self.linear2    = nn.Linear(d_model * 4, d_model)\n","        # Layer normalization for residual connections\n","        self.norm1      = nn.LayerNorm(d_model)\n","        self.norm2      = nn.LayerNorm(d_model)\n","        self.norm3      = nn.LayerNorm(d_model)\n","        # Dropout for regularization\n","        self.dropout    = nn.Dropout(dropout)\n","\n","    def forward(self, tgt: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass through the cross-attention block.\n","\n","        Args:\n","            tgt (Tensor): Target embeddings of shape (T, B, D)\n","                          where T=target sequence length,\n","                                B=batch size,\n","                                D=embedding dimension.\n","            memory (Tensor): Source/memory embeddings of shape (S, B, D)\n","                              where S=source sequence length.\n","\n","        Returns:\n","            Tensor: Output embeddings of shape (T, B, D).\n","        \"\"\"\n","        # ----- 1) Self-Attention on target -----#\n","        # q, k, v all come from tgt\n","        #Each target looks at all other targets in the SAME sequence\n","        tgt2, _ = self.self_attn(tgt, tgt, tgt)\n","        # Residual connection + normalization\n","        tgt = tgt + self.dropout(tgt2)\n","        tgt = self.norm1(tgt)\n","\n","        # ----- 2) Cross-Attention: tgt queries, memory keys/values -----#\n","        # The query comes from the updated target (after self attention)\n","        # The key and value comes from the memory / source / the encoder's output\n","        tgt2, _ = self.cross_attn(query=tgt, key=memory, value=memory)\n","        # Residual connection + normalization\n","        tgt = tgt + self.dropout(tgt2)\n","        tgt = self.norm2(tgt)\n","\n","        # ----- 3) Feed-Forward Network -----#\n","        # A 2 layer multilayer perceptron (MLP)\n","        # The hidden layer (linear1) has a ReLU and dropout\n","        # The second linear (lienar2) reconstructs the matrix back\n","        # to the original dimensions. If you look at the CrossAttentionBlock class,\n","        # linear1 takes an input size d_model and the output is of size d_model*4\n","        # linear2 takes an input size of d_model*4 and has an output size of d_model\n","        ff = self.linear2(self.dropout(nn.functional.relu(self.linear1(tgt))))\n","        # Residual + normalization\n","        tgt = tgt + self.dropout(ff)\n","        tgt = self.norm3(tgt)\n","\n","        return tgt\n","\n","\n","def demo_cross_attention():\n","    \"\"\"\n","    Demonstrates the CrossAttentionBlock with random tensors.\n","    \"\"\"\n","    # Configuration\n","    batch_size      = 2      # number of examples in a batch\n","    seq_len_source  = 5      # length of the source (memory) sequence\n","    seq_len_target  = 3      # length of the target sequence\n","    d_model         = 16     # embedding size\\ n\n","    nhead           = 4      # number of attention heads\n","\n","    # Create random source and target sequences\n","    # PyTorch MultiheadAttention expects shape (seq_len, batch, embed_dim)\n","    source = torch.randn(seq_len_source, batch_size, d_model)\n","    target = torch.randn(seq_len_target, batch_size, d_model)\n","\n","    # Instantiate the cross-attention block\n","    cross_block = CrossAttentionBlock(d_model=d_model, nhead=nhead)\n","\n","    # Forward pass: target attends to source\n","    output = cross_block(target, source)\n","\n","    print(f\"Input target shape: {target.shape}\")\n","    print(f\"Input source shape: {source.shape}\")\n","    print(f\"Output embeddings shape: {output.shape}\")"],"metadata":{"id":"vbG3-X4-HBYg","executionInfo":{"status":"ok","timestamp":1746592809442,"user_tz":-480,"elapsed":16,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Now, let's run demo_cross_attention(). In `source`, it generates 5 “memory” positions, batch size 2, hidden dim 16. In `target`, it generates 3 “query” positions, batch size 2, hidden dim 16.\n","\n","We then run cross attention. It self-atttends over the 3 target vectors, cross‑attends each of those 3 positions over the 5 source vectors, then passes the result through the feed‑forward network.\n","\n","We want to see that the output has the same shape as the target."],"metadata":{"id":"89-G7r-mUsND"}},{"cell_type":"code","source":["demo_cross_attention()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-sd7jsxWHujx","executionInfo":{"status":"ok","timestamp":1746588785304,"user_tz":-480,"elapsed":164,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}},"outputId":"e093e5c2-0999-4aff-a0ec-93f321e4a5b6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Input target shape: torch.Size([3, 2, 16])\n","Input source shape: torch.Size([5, 2, 16])\n","Output embeddings shape: torch.Size([3, 2, 16])\n"]}]},{"cell_type":"markdown","source":["## A cross attention transformer in the context of ASR, and a simple training run.\n","\n","In the following code, we use pytorch's built in TransformerEncoder and TransformerDecoder method that already implements cross attention.\n","Just like above, we do the following:\n","1. Self attention on the encoder input AND the decoder input\n","2. Cross-attention against the encoder memory (which refers to the encoder output)\n","3. A position-wise feed forward neural network (position-wise refers to the idea of not mixing information at each index of the matrix, unlike cross-attention which gives cross-token context. It is a traditional MLP which does non-linear transformation per-position.)\n","4. Residual connections and layer norm at every step.\n","\n","\n"],"metadata":{"id":"RblguAc-HwfK"}},{"cell_type":"code","source":["!pip install soundfile librosa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCRCxW3_Ocmn","executionInfo":{"status":"ok","timestamp":1746588788812,"user_tz":-480,"elapsed":3507,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}},"outputId":"e0196ff1-3623-4ba4-ab47-611b842e108d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (2.0.2)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.2)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","from torchaudio.transforms import MelSpectrogram\n","from jiwer import wer\n","\n","# -----------------------------------------------------------------------------\n","# 1) Data preparation with HuggingFace datasets\n","# -----------------------------------------------------------------------------\n","ds = load_dataset(\n","    \"hf-internal-testing/librispeech_asr_demo\",\n","    \"clean\",\n","    split=\"validation\",\n","    trust_remote_code=True\n",")\n","\n","# Do an 80/20 split for train/val\n","splits = ds.train_test_split(test_size=0.2, seed=42)\n","train_ds, val_ds = splits[\"train\"], splits[\"test\"]\n","\n","# Character tokenizer\n","chars   = list(\"abcdefghijklmnopqrstuvwxyz' \")\n","char2idx = {c: i+1 for i,c in enumerate(chars)}  # 0 reserved for padding\n","vocab_size = len(char2idx) + 1\n","\n","mel_transform = MelSpectrogram(sample_rate=16_000, n_mels=128)\n","\n","def collate_batch(batch):\n","    specs, labels = [], []\n","    for ex in batch:\n","        # Make sure we load as float32, not the default float64\n","        # MelSpectrogram expects float32\n","        waveform = torch.tensor(ex[\"audio\"][\"array\"], dtype=torch.float32)\n","\n","        # Now the mel transform will run without complaint\n","        spec = mel_transform(waveform.unsqueeze(0))      # -> (1, n_mels, T)\n","        spec = spec.squeeze(0).transpose(0,1)            # -> (T, n_mels)\n","        specs.append(spec)\n","\n","        # Text → token IDs (same as before)\n","        ids = torch.tensor([char2idx.get(c,0) for c in ex[\"text\"].lower()],\n","                           dtype=torch.long)\n","        labels.append(ids)\n","\n","    # Pad and batch\n","    specs = nn.utils.rnn.pad_sequence(specs, batch_first=True)   # (B, T, n_mels)\n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True) # (B, L)\n","    return specs, labels\n","\n","train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, \\\n","                      collate_fn=collate_batch)\n","val_dl   = DataLoader(val_ds,   batch_size=4, shuffle=False, \\\n","                      collate_fn=collate_batch)\n","\n","\n","# -----------------------------------------------------------------------------\n","# 2) Build a tiny encoder–decoder ASR model\n","# -----------------------------------------------------------------------------\n","class ASRModel(nn.Module):\n","    def __init__(self, d_model=256, nhead=8, num_layers=3):\n","        super().__init__()\n","        # audio encoder: project 128→d_model, then self-attend\n","        self.audio_proj = nn.Linear(128, d_model)\n","        enc_layer = nn.TransformerEncoderLayer(d_model, nhead) #8 head multi-head self attention\n","        self.encoder  = nn.TransformerEncoder(enc_layer, num_layers)\n","\n","        # text decoder: embed + cross‑attention blocks\n","        self.text_emb  = nn.Embedding(vocab_size, d_model)\n","        dec_layer      = nn.TransformerDecoderLayer(d_model, nhead) #8 head multi-head self attention\n","        self.decoder   = nn.TransformerDecoder(dec_layer, num_layers)\n","\n","        # final output projection\n","        self.out_proj  = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, specs, tokens_in):\n","        \"\"\"\n","        specs: (B, T_src, 128)\n","        tokens_in: (B, T_tgt)  — teacher‑forcing inputs\n","        \"\"\"\n","        # Encode audio\n","        x = self.audio_proj(specs)               # (B, T_src, d_model)\n","        x = x.permute(1,0,2)                     # (T_src, B, d_model)\n","        memory = self.encoder(x)                 # same shape\n","\n","        # Prepare decoder input\n","        y = self.text_emb(tokens_in)             # (B, T_tgt, d_model)\n","        y = y.permute(1,0,2)                     # (T_tgt, B, d_model)\n","        out = self.decoder(y, memory)            # (T_tgt, B, d_model)\n","        out = out.permute(1,0,2)                 # (B, T_tgt, d_model)\n","\n","        logits = self.out_proj(out)              # (B, T_tgt, vocab_size)\n","        return logits\n","\n","\n","# -----------------------------------------------------------------------------\n","# 3) Training loop\n","# -----------------------------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = ASRModel().to(device)\n","optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","def train_epoch(dl):\n","    model.train()\n","    total_loss = 0\n","    for specs, labels in dl:\n","        specs, labels = specs.to(device), labels.to(device)\n","        # decoder input: all but last token\n","        inp = labels[:, :-1]\n","        tgt = labels[:, 1:]\n","        logits = model(specs, inp)            # (B, T-1, V)\n","        loss = criterion(logits.reshape(-1, logits.size(-1)),\n","                         tgt.reshape(-1))\n","        optim.zero_grad(); loss.backward(); optim.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dl)\n","\n","def eval_epoch(dl):\n","    model.eval()\n","    total_loss = 0\n","    preds, refs = [], []\n","    with torch.no_grad():\n","        for specs, labels in dl:\n","            specs, labels = specs.to(device), labels.to(device)\n","            inp = labels[:, :-1]\n","            tgt = labels[:, 1:]\n","            logits = model(specs, inp)\n","            loss = criterion(logits.reshape(-1, logits.size(-1)),\n","                             tgt.reshape(-1))\n","            total_loss += loss.item()\n","\n","            # greedy decode\n","            out_ids = logits.argmax(-1).cpu().tolist()\n","            ref_ids = tgt.cpu().tolist()\n","            for o, r in zip(out_ids, ref_ids):\n","                preds.append(\"\".join(chars[i-1] for i in o if i>0))\n","                refs.append(\"\".join(chars[i-1] for i in r if i>0))\n","\n","    avg_loss = total_loss / len(dl)\n","    avg_wer  = wer(refs, preds)\n","    return avg_loss, avg_wer\n","\n","# run 20 epochs for demo\n","# You can see the train_loss and val_loss decreasing\n","# FOR WARREN: IDK WHY THE val_WER not decreasing\n","for epoch in range(1,21):\n","    train_loss = train_epoch(train_dl)\n","    val_loss, val_wer = eval_epoch(val_dl)\n","    print(f\"Epoch {epoch} ▶ train_loss={train_loss:.3f}  val_loss={val_loss:.3f}  val_WER={val_wer:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zL1259NlHv1k","executionInfo":{"status":"ok","timestamp":1746592883148,"user_tz":-480,"elapsed":33091,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}},"outputId":"de98b503-f7f9-4368-aea6-e7e0b329ef5b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 ▶ train_loss=2.931  val_loss=2.757  val_WER=1.412\n","Epoch 2 ▶ train_loss=2.626  val_loss=2.585  val_WER=1.255\n","Epoch 3 ▶ train_loss=2.470  val_loss=2.518  val_WER=1.278\n","Epoch 4 ▶ train_loss=2.403  val_loss=2.482  val_WER=1.176\n","Epoch 5 ▶ train_loss=2.380  val_loss=2.459  val_WER=1.188\n","Epoch 6 ▶ train_loss=2.339  val_loss=2.456  val_WER=1.212\n","Epoch 7 ▶ train_loss=2.318  val_loss=2.448  val_WER=1.251\n","Epoch 8 ▶ train_loss=2.303  val_loss=2.452  val_WER=1.145\n","Epoch 9 ▶ train_loss=2.284  val_loss=2.445  val_WER=1.224\n","Epoch 10 ▶ train_loss=2.260  val_loss=2.444  val_WER=1.161\n","Epoch 11 ▶ train_loss=2.236  val_loss=2.448  val_WER=1.235\n","Epoch 12 ▶ train_loss=2.219  val_loss=2.448  val_WER=1.200\n","Epoch 13 ▶ train_loss=2.204  val_loss=2.451  val_WER=1.192\n","Epoch 14 ▶ train_loss=2.188  val_loss=2.463  val_WER=1.231\n","Epoch 15 ▶ train_loss=2.158  val_loss=2.470  val_WER=1.204\n","Epoch 16 ▶ train_loss=2.141  val_loss=2.474  val_WER=1.204\n","Epoch 17 ▶ train_loss=2.117  val_loss=2.478  val_WER=1.329\n","Epoch 18 ▶ train_loss=2.102  val_loss=2.494  val_WER=1.333\n","Epoch 19 ▶ train_loss=2.070  val_loss=2.514  val_WER=1.180\n","Epoch 20 ▶ train_loss=2.055  val_loss=2.526  val_WER=1.341\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"B9NMATmt8vSD","executionInfo":{"status":"ok","timestamp":1746588852596,"user_tz":-480,"elapsed":5,"user":{"displayName":"Warren Low","userId":"03832505700203327791"}}},"execution_count":5,"outputs":[]}]}